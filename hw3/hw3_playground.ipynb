{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f3277b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from activations import (\n",
    "    sigmoid, softmax, relu, identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40199a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_0_hidden_layer(x_NF, w_arr, b_arr, output_activation):\n",
    "    \"\"\" Make predictions for a neural net with no hidden layers\n",
    "\n",
    "    This function demonstrates 3 special cases for an MLP with 0 hidden layers\n",
    "    1. identity activation : equivalent to linear regression\n",
    "    2. sigmoid activation : equivalent to binary logistic regression\n",
    "    3. softmax activation : equivalent to multi-class logistic regression\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x_NF : 2D numpy array, shape (N, F) = (n_samples, n_features)\n",
    "        Input features\n",
    "\n",
    "    w_arr : 1D or 2D numpy array, shape (n_features, n_outputs)\n",
    "        For single output, this may be a 1D array of shape (n_features,)\n",
    "\n",
    "    b_arr : 1D numpy array, shape (n_outputs,)\n",
    "        For single output, this may be a scalar float\n",
    "\n",
    "    output_activation : callable\n",
    "        Activation function for the output layer.\n",
    "        Given an input array, must return output array of same shape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yhat_NC : 1D or 2D numpy array:\n",
    "        shape (N,C) = (n_samples, n_outputs) if n_outputs > 1, else shape (N,C) = (n_samples,)\n",
    "        Predicted values using the specified neural network configuration\n",
    "        \n",
    "        Suppose we had N=3 examples, F=1 features, and n_outputs = 1\n",
    "        * if output_activation == identity, return array of real values\n",
    "            e.g., input: [x1, x2, x3] --> output:[2.5, -6.7, 12]\n",
    "        * if output_activation == sigmoid, return an array of probabilities\n",
    "            e.g., input: [x1, x2, x3] --> output:[0.3, 0.8, 1.0]\n",
    "\n",
    "        Suppose we had N=2 examples, F=1 features, and n_outputs = 3\n",
    "        * if output_activation == softmax, return an array of proba vectors.\n",
    "            e.g., input: [x1, x2] --> output:[[0.2, 0.4, 0.4], [0.8, 0.2, 0.]]\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    See doctest_neural_nets.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return output_activation(np.dot(x_NF, w_arr) + b_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "993d6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87125ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "x_NF, y_N = sklearn.datasets.make_regression(n_samples=100, n_features=5, noise=1, random_state=42)\n",
    "reg = sklearn.linear_model.LinearRegression().fit(x_NF, y_N)\n",
    "w = reg.coef_\n",
    "#print(w.shape)\n",
    "b = reg.intercept_\n",
    "round(b, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "163f0c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "yhat_N = predict_0_hidden_layer(x_NF, w, b, output_activation=identity)\n",
    "print(yhat_N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f53350b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(yhat_N, reg.predict(x_NF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b8cb517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_NF, y_N = sklearn.datasets.make_classification(n_samples=100, n_features=5, n_classes=2, random_state=42)\n",
    "clf = sklearn.linear_model.LogisticRegression().fit(x_NF, y_N)\n",
    "w = np.squeeze(clf.coef_)\n",
    "\n",
    "b = clf.intercept_[0]\n",
    "round(b, 3)\n",
    "\n",
    "yproba1_N = predict_0_hidden_layer(x_NF, w, b, output_activation=sigmoid)\n",
    "np.allclose(yproba1_N, clf.predict_proba(x_NF)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f82158dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_NF, y_N = sklearn.datasets.make_classification(n_samples=100, n_features=5, n_informative=3, n_classes=4, random_state=42)\n",
    "multi_clf = sklearn.linear_model.LogisticRegression(multi_class=\"multinomial\").fit(x_NF, y_N)\n",
    "w = multi_clf.coef_.T\n",
    "\n",
    "b = multi_clf.intercept_\n",
    "\n",
    "\n",
    "yproba_NC = predict_0_hidden_layer(x_NF, w, b, output_activation=softmax)\n",
    "\n",
    "np.allclose(yproba_NC, multi_clf.predict_proba(x_NF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea0ef468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_n_hidden_layer(\n",
    "        x_NF, w_list, b_list,\n",
    "        hidden_activation=relu, output_activation=softmax):\n",
    "    \"\"\" Make predictions for an MLP with zero or more hidden layers\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_NF : numpy array of shape (n_samples, n_features)\n",
    "        Input data for prediction.\n",
    "\n",
    "    w_list : list of numpy array, length is n_layers\n",
    "        Each entry represents 2D weight array for corresponding layer\n",
    "        Shape of each entry is (n_inputs, n_outputs)\n",
    "        Layers are ordered from input to output in predictive order\n",
    "\n",
    "    b_list : list of numpy array, length is n_layers\n",
    "        Each entry represents the intercept aka bias array for a specific layer\n",
    "        Shape of each entry is (n_outputs,)\n",
    "        Layers are ordered from input to output in predictive order\n",
    "\n",
    "    hidden_activation : callable, optional (default=relu)\n",
    "        Activation function for all hidden layers.\n",
    "\n",
    "    output_activation : callable, optional (default=softmax)\n",
    "        Activation function for the output layer.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    yhat_NC : 1D or 2D numpy array:\n",
    "        shape (N,C) = (n_samples, n_outputs) if n_outputs > 1, else shape (N,C) = (n_samples,)\n",
    "        Predicted values (for regression) or probabilities (if classification)\n",
    "        Each row corresponds to corresponding row of x_NF input array.\n",
    "\n",
    "        Suppose we had N=2 examples, F=1 features, and n_outputs = 1\n",
    "        * if output_activation == sigmoid, return an array of proba vectors of label 1.\n",
    "            e.g., input: [x1, x2] --> output:[[0.2], [0.8]]\n",
    "\n",
    "        Suppose we had N=2 examples, F=1 features, and n_outputs = 3\n",
    "        * if output_activation == softmax, return an array of proba vectors.\n",
    "            e.g., input: [x1, x2] --> output:[[0.2, 0.4, 0.4], [0.8, 0.2, 0.]]\n",
    "\n",
    "    Examples\n",
    "    _______\n",
    "    See doctest_neural_nets.py\n",
    "    \"\"\"\n",
    "    n_layers = len(w_list)\n",
    "    assert n_layers == len(b_list)\n",
    "\n",
    "    # Forward propagation: start from the input layer\n",
    "    out_arr = x_NF\n",
    "\n",
    "    for layer_id in range(n_layers):\n",
    "        # Get w and b arrays for current layer\n",
    "        w_arr = w_list[layer_id]\n",
    "        b_arr = b_list[layer_id]\n",
    "\n",
    "        # Perform the linear operation: X Â· w + b\n",
    "        out_arr = np.dot(out_arr, w_arr) + b_arr\n",
    "\n",
    "        # Perform the non-linear activation of current layer\n",
    "        if (layer_id == (n_layers - 1)):\n",
    "            out_arr = output_activation(out_arr)\n",
    "        else:\n",
    "            out_arr = hidden_activation(out_arr)\n",
    "    \n",
    "    #print(\"shape is\", out_arr.shape)\n",
    "    out_arr = np.squeeze(out_arr)  # reduce unnecessary dimension for single output\n",
    "    #print(\"shape is\", out_arr.shape)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "529c26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ef1f7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "\n",
    "# from neural_networks import (\n",
    "# \tpredict_0_hidden_layer, predict_n_hidden_layer,\n",
    "# \tidentity, sigmoid, softmax, relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "933fa95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Test Cases for predict_n_hidden_layers\n",
    "# --------------------------------------\n",
    "\n",
    "# 1. predict probabilities for all classes\n",
    "\n",
    "x_NF, y_N = sklearn.datasets.make_classification(n_samples=100, n_features=5, n_informative=3, n_classes=4, random_state=42)\n",
    "x_NF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bdd296bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2hidden = sklearn.neural_network.MLPClassifier(\\\n",
    "    hidden_layer_sizes=[2],activation='relu', solver='lbfgs', random_state=1)\n",
    "mlp_2hidden = mlp_2hidden.fit(x_NF, y_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "01ec5b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yproba_N2 = predict_n_hidden_layer(x_NF, mlp_2hidden.coefs_, mlp_2hidden.intercepts_)\n",
    "yproba_N2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2601af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85, 0.05, 0.1 , 0.  ],\n",
       "       [0.97, 0.02, 0.02, 0.  ]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(yproba_N2[:2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37c95373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(yproba_N2[:2], 2)\n",
    "# array([[0.85, 0.05, 0.1 , 0.  ],\n",
    "#        [0.97, 0.02, 0.02, 0.  ]])\n",
    "# >>> print(np.sum(yproba_N2[:2], axis=1))\n",
    "# [1. 1.]\n",
    "ideal_yproba_N2 = mlp_2hidden.predict_proba(x_NF)\n",
    "ideal_yproba_N2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ff01d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(yproba_N2, ideal_yproba_N2)\n",
    "# >>> np.round(ideal_yproba_N2[:2], 2)\n",
    "# array([[0.85, 0.05, 0.1 , 0.  ],\n",
    "#        [0.97, 0.02, 0.02, 0.  ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "510d1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "yhat_N2 = predict_n_hidden_layer(x_NF,\n",
    "    mlp_2hidden.coefs_, mlp_2hidden.intercepts_, output_activation=identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd83ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
